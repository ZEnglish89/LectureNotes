
Changing variables:
Sometimes a measurement will be difficult, but another, related measurement will be much easier.
You can take the easier measurement and work with the correlation between the two instead.

A classic example is the positive correlation between the height and weight of an animal, using height as a calculation of volume and working with average density to get a weight estimate.

For all Expected values: $$E[g(x)] = \Sigma g(k) \cdot PMF(k)$$ or $$\int{g(k) \cdot PDF(k)}$$ for discrete and continuous respectively. "Value" multiplied by "probability of that value." The integral bounds will be contextual based on the distribution and situation. If there's nothing to determine them by, $(-\infty,\infty)$ will often work out.


"Expected value" and "mean" are synonyms.

If X and Y are [[Random Variable]] and c is a constant:
$E[X+Y]=E[X]+E[Y]$ and $E[cX]=cE[X]$
